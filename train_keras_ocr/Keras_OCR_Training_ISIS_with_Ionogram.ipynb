{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d00c821",
   "metadata": {},
   "source": [
    "# Train Keras OCR with Ionograms\n",
    "\n",
    "In the last Notebook, I tried to train Keras OCR with synthetic data, but the results weren't conclusive. The goal here is to train Keras OCR with the text that is already on the ionograms. Here are the refences :\n",
    "https://keras-ocr.readthedocs.io/en/latest/examples/fine_tuning_recognizer.html\n",
    "https://stackoverflow.com/questions/52004133/how-to-improve-image-quality\n",
    "https://stackoverflow.com/questions/39308030/how-do-i-increase-the-contrast-of-an-image-in-python-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'L:/DATA/ISIS/keras_ocr/'\n",
    "\n",
    "import os\n",
    "import math\n",
    "import imgaug\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "import keras_ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e56eca",
   "metadata": {},
   "source": [
    "## Train the recognizer\n",
    "After some research, training the detector seems hard and time consuming, so we'll start by training the recognizer with our own pictures. To train the recognizer, we need small images of digits from ionograms. I cropped 60 ionograms and their metadata. After doing some tests, I came to realize that the detector was working better when the images are only black and white, so I did a little bit of pre-processing (see the Notebook *Pre-process training images*). Here how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2471c75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d53c63c1f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = plt.imread(data_dir+'test_images/contrast/20-5.jpg')\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf68123",
   "metadata": {},
   "source": [
    "I also created a txt file with the name of the images and the right numbers. It looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c4de89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>empty</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...</td>\n",
       "      <td>None</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...</td>\n",
       "      <td>None</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...</td>\n",
       "      <td>None</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...</td>\n",
       "      <td>None</td>\n",
       "      <td>0354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...</td>\n",
       "      <td>None</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file empty   text\n",
       "0  L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...  None     36\n",
       "1  L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...  None     00\n",
       "2  L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...  None     69\n",
       "3  L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...  None     11\n",
       "4  L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...  None   0354\n",
       "5  L:/DATA/ISIS/keras_ocr/test_images/contrast/1-...  None     35"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(data_dir+'test_images/contrast/annotations.txt', header=None, \n",
    "                      names=['file','text'], dtype={'file' : str, 'text' : str})\n",
    "dataset['file'] = data_dir+'test_images/contrast/' + dataset['file']\n",
    "dataset['text'] = dataset['text'].astype(str)\n",
    "dataset.insert(1, 'empty', None)\n",
    "\n",
    "dataset.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e9a58e",
   "metadata": {},
   "source": [
    "We now divide the images for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b4801ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset, test_subset = sklearn.model_selection.train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "train_subset.reset_index(inplace=True, drop=True)\n",
    "test_subset.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train = []\n",
    "for i in range(len(train_subset)):\n",
    "    train.append(tuple(train_subset.loc[i].to_list()))\n",
    "\n",
    "test = []\n",
    "for i in range(len(test_subset)):\n",
    "    test.append(tuple(test_subset.loc[i].to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13628869",
   "metadata": {},
   "source": [
    "We specify that our alphabet is only numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd697fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided alphabet does not match pretrained alphabet. Using backbone weights only.\n",
      "Looking for C:\\Users\\mfortier\\.keras-ocr\\crnn_kurapan_notop.h5\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "recognizer = keras_ocr.recognition.Recognizer(alphabet=string.digits)\n",
    "recognizer.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa417e0",
   "metadata": {},
   "source": [
    "We divide the training images in training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2da0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "augmenter = imgaug.augmenters.Sequential([\n",
    "    imgaug.augmenters.GammaContrast(gamma=(0.25, 3.0)),\n",
    "])\n",
    "\n",
    "(training_image_gen, training_steps), (validation_image_gen, validation_steps) = [\n",
    "    (\n",
    "        keras_ocr.datasets.get_recognizer_image_generator(\n",
    "            labels=labels,\n",
    "            height=recognizer.model.input_shape[1],\n",
    "            width=recognizer.model.input_shape[2],\n",
    "            alphabet=recognizer.alphabet,\n",
    "            augmenter=augmenter\n",
    "        ),\n",
    "        len(labels) // batch_size\n",
    "    ) for labels, augmenter in [(train, augmenter), (test, None)]\n",
    "]\n",
    "training_gen, validation_gen = [\n",
    "    recognizer.get_batch_generator(\n",
    "        image_generator=image_generator,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    for image_generator in [training_image_gen, validation_image_gen]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "539a2e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 / 252 instances have illegal characters.\n",
      "text: 0354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d5602ff370>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, text = next(training_image_gen)\n",
    "print('text:', text)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4f41e",
   "metadata": {},
   "source": [
    "Time for the training to begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b408417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "252/252 [==============================] - ETA: 0s - loss: 7.331563 / 63 instances have illegal characters.\n",
      "252/252 [==============================] - 38s 102ms/step - loss: 7.3315 - val_loss: 7.2346\n",
      "Epoch 2/1000\n",
      "252/252 [==============================] - 21s 85ms/step - loss: 6.2646 - val_loss: 6.1809\n",
      "Epoch 3/1000\n",
      "252/252 [==============================] - 23s 92ms/step - loss: 5.4370 - val_loss: 6.1032\n",
      "Epoch 4/1000\n",
      "252/252 [==============================] - 22s 89ms/step - loss: 4.8658 - val_loss: 4.6592\n",
      "Epoch 5/1000\n",
      "252/252 [==============================] - 22s 87ms/step - loss: 4.0655 - val_loss: 4.7451\n",
      "Epoch 6/1000\n",
      "252/252 [==============================] - 24s 95ms/step - loss: 3.3568 - val_loss: 4.1650\n",
      "Epoch 7/1000\n",
      "252/252 [==============================] - 24s 95ms/step - loss: 3.0081 - val_loss: 6.1425\n",
      "Epoch 8/1000\n",
      "252/252 [==============================] - 25s 99ms/step - loss: 2.4006 - val_loss: 3.0249\n",
      "Epoch 9/1000\n",
      "252/252 [==============================] - 21s 85ms/step - loss: 2.0011 - val_loss: 2.8361\n",
      "Epoch 10/1000\n",
      "252/252 [==============================] - 22s 88ms/step - loss: 1.6266 - val_loss: 1.9741\n",
      "Epoch 11/1000\n",
      "252/252 [==============================] - 22s 87ms/step - loss: 1.4896 - val_loss: 1.8879\n",
      "Epoch 12/1000\n",
      "252/252 [==============================] - 23s 91ms/step - loss: 1.6743 - val_loss: 2.3353\n",
      "Epoch 13/1000\n",
      "252/252 [==============================] - 24s 94ms/step - loss: 1.3733 - val_loss: 2.7954\n",
      "Epoch 14/1000\n",
      "252/252 [==============================] - 21s 84ms/step - loss: 1.3063 - val_loss: 2.6845\n",
      "Epoch 15/1000\n",
      "252/252 [==============================] - 21s 84ms/step - loss: 1.0650 - val_loss: 3.5114\n",
      "Epoch 16/1000\n",
      "252/252 [==============================] - 22s 87ms/step - loss: 0.8233 - val_loss: 1.8100\n",
      "Epoch 17/1000\n",
      "252/252 [==============================] - 21s 84ms/step - loss: 1.1742 - val_loss: 2.9144\n",
      "Epoch 18/1000\n",
      "252/252 [==============================] - 23s 91ms/step - loss: 1.0781 - val_loss: 2.5561\n",
      "Epoch 19/1000\n",
      "252/252 [==============================] - 23s 92ms/step - loss: 0.8516 - val_loss: 2.1420\n",
      "Epoch 20/1000\n",
      "252/252 [==============================] - 22s 88ms/step - loss: 0.6763 - val_loss: 1.4432\n",
      "Epoch 21/1000\n",
      "252/252 [==============================] - 23s 90ms/step - loss: 0.7349 - val_loss: 3.2090\n",
      "Epoch 22/1000\n",
      "252/252 [==============================] - 22s 89ms/step - loss: 0.5637 - val_loss: 1.0682\n",
      "Epoch 23/1000\n",
      "252/252 [==============================] - 22s 88ms/step - loss: 0.3533 - val_loss: 1.7359\n",
      "Epoch 24/1000\n",
      "252/252 [==============================] - 23s 93ms/step - loss: 0.6850 - val_loss: 1.6566\n",
      "Epoch 25/1000\n",
      "252/252 [==============================] - 23s 91ms/step - loss: 0.5933 - val_loss: 1.0063\n",
      "Epoch 26/1000\n",
      "252/252 [==============================] - 23s 90ms/step - loss: 0.4768 - val_loss: 1.4983\n",
      "Epoch 27/1000\n",
      "252/252 [==============================] - 23s 91ms/step - loss: 0.5893 - val_loss: 1.6788\n",
      "Epoch 28/1000\n",
      "252/252 [==============================] - 22s 88ms/step - loss: 0.4937 - val_loss: 2.0292\n",
      "Epoch 29/1000\n",
      "252/252 [==============================] - 22s 88ms/step - loss: 0.5576 - val_loss: 1.6645\n",
      "Epoch 30/1000\n",
      "252/252 [==============================] - 24s 95ms/step - loss: 0.4156 - val_loss: 1.5992\n",
      "Epoch 31/1000\n",
      "252/252 [==============================] - 21s 84ms/step - loss: 0.3465 - val_loss: 0.8808\n",
      "Epoch 32/1000\n",
      "252/252 [==============================] - 22s 88ms/step - loss: 0.5356 - val_loss: 1.8916\n",
      "Epoch 33/1000\n",
      "252/252 [==============================] - 24s 96ms/step - loss: 0.2547 - val_loss: 1.6166\n",
      "Epoch 34/1000\n",
      "252/252 [==============================] - 24s 94ms/step - loss: 0.3843 - val_loss: 2.2033\n",
      "Epoch 35/1000\n",
      "252/252 [==============================] - 23s 90ms/step - loss: 0.4679 - val_loss: 2.5121\n",
      "Epoch 36/1000\n",
      "252/252 [==============================] - 24s 95ms/step - loss: 0.2588 - val_loss: 2.4196\n",
      "Epoch 37/1000\n",
      "252/252 [==============================] - 23s 89ms/step - loss: 0.2634 - val_loss: 2.1460\n",
      "Epoch 38/1000\n",
      "252/252 [==============================] - 23s 90ms/step - loss: 0.3112 - val_loss: 1.7689\n",
      "Epoch 39/1000\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.3938"
     ]
    }
   ],
   "source": [
    "recognizer_basepath = os.path.join('L:/DATA/ISIS/keras_ocr/', f'recognizer_{datetime.datetime.now().isoformat()}')\n",
    "callbacks=[\n",
    "    tf.keras.callbacks.EarlyStopping(restore_best_weights=False, patience=10),\n",
    "    tf.keras.callbacks.CSVLogger(f'{recognizer_basepath}.csv', append=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=f'{recognizer_basepath}.h5')        \n",
    "    ]\n",
    "recognizer.training_model.fit(\n",
    "    training_gen,\n",
    "    steps_per_epoch=training_steps,\n",
    "    validation_steps=validation_steps,\n",
    "    validation_data=validation_gen,\n",
    "    callbacks=callbacks,\n",
    "    epochs=1000,\n",
    "    workers=0,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959a4fb",
   "metadata": {},
   "source": [
    "Test a couple of images with our improved recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c995e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filepath, _, actual = test[0]\n",
    "predicted = recognizer.recognize(image_filepath)\n",
    "print(f'Predicted: {predicted}, Actual: {actual}')\n",
    "_ = plt.imshow(keras_ocr.tools.read(image_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c33261",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filepath, _, actual = test[1]\n",
    "predicted = recognizer.recognize(image_filepath)\n",
    "print(f'Predicted: {predicted}, Actual: {actual}')\n",
    "_ = plt.imshow(keras_ocr.tools.read(image_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filepath, _, actual = test[2]\n",
    "predicted = recognizer.recognize(image_filepath)\n",
    "print(f'Predicted: {predicted}, Actual: {actual}')\n",
    "_ = plt.imshow(keras_ocr.tools.read(image_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f9734",
   "metadata": {},
   "source": [
    "Let's test it on real ionograms picked at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "def draw_random_subdir(dataDir):\n",
    "    directory_list = os.listdir(dataDir)\n",
    "    directory = directory_list[randrange(len(directory_list))]\n",
    "    subdirectory_list = os.listdir(dataDir + directory + '/')\n",
    "    subdirectory = subdirectory_list[randrange(len(subdirectory_list))]\n",
    "    image_list = os.listdir(dataDir + directory + '/'+subdirectory+'/')\n",
    "    image = image_list[randrange(len(image_list))]    \n",
    "    return directory, subdirectory, image\n",
    "\n",
    "dataDir = 'L:/DATA/ISIS/raw_upload_20230421/'\n",
    "img_list=[]\n",
    "for i in range(3):\n",
    "    directory, subdirectory, image = draw_random_subdir(dataDir)\n",
    "    img_list.append(dataDir + directory + '/' + subdirectory + '/' + image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b0733",
   "metadata": {},
   "source": [
    "We need to apply the same filter I did on the cropped images used to train keras ocr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "new_img = []\n",
    "# Change the colors\n",
    "black = (0,0,0)\n",
    "white = (255,255,255)\n",
    "threshold = (85,85,85)\n",
    "\n",
    "for img in img_list :\n",
    "    # Open input image in grayscale mode and get its pixels.\n",
    "    image = Image.open(img).convert(\"LA\")\n",
    "    pixels = image.getdata()\n",
    "    newPixels = []\n",
    "\n",
    "    # Compare each pixel \n",
    "    for pixel in pixels:\n",
    "        if pixel < threshold:\n",
    "            newPixels.append(black)\n",
    "        else:\n",
    "            newPixels.append(white)\n",
    "\n",
    "    # Create and save new image.\n",
    "    image = Image.new(\"RGB\",image.size)\n",
    "    image.putdata(newPixels)\n",
    "    new_img.append(np.array(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e8813",
   "metadata": {},
   "source": [
    "Pass the images to the recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = keras_ocr.pipeline.Pipeline(recognizer=recognizer)\n",
    "prediction_groups = pipeline.recognize(new_img)\n",
    "# plot the text predictions\n",
    "fig, axs = plt.subplots(nrows=len(new_img), figsize=(15, 10))\n",
    "for ax, image, predictions in zip(axs, new_img, prediction_groups):\n",
    "    keras_ocr.tools.drawAnnotations(image=image, \n",
    "                                    predictions=predictions, \n",
    "                                    ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1b3d37",
   "metadata": {},
   "source": [
    "Let's see the results if the images are cropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c58c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_img=[]\n",
    "for i,im in enumerate(new_img) :\n",
    "    height, width, _ = im.shape\n",
    "    im = im[int(height-height*0.15):height,0:width]\n",
    "    crop_img.append(im)\n",
    "prediction_groups = pipeline.recognize(crop_img)\n",
    "# plot the text predictions\n",
    "fig, axs = plt.subplots(nrows=len(crop_img), figsize=(15, 7))\n",
    "for ax, image, predictions in zip(axs, crop_img, prediction_groups):\n",
    "    keras_ocr.tools.drawAnnotations(image=image, \n",
    "                                    predictions=predictions, \n",
    "                                    ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c13518",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d359c4",
   "metadata": {},
   "source": [
    "Save the trained recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer.model.save_weights(data_dir +'ISIS_reading_contrast.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
